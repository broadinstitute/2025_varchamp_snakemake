{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af2f74be-9dec-469e-a1b0-f8cc5daecf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classification pipeline\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from typing import Union\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "from functools import partial\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"..\")\n",
    "from utils import find_feat_cols, find_meta_cols, remove_nan_infs_columns\n",
    "\n",
    "## constants\n",
    "FEAT_TYPE_SET = set([\"GFP\", \"DNA\", \"AGP\", \"Mito\", \"TxControl\", \"Morph\"])\n",
    "\n",
    "\"\"\"\n",
    "    Util functions for annotations\n",
    "\"\"\"\n",
    "def control_type_helper(col_annot: str):\n",
    "    \"\"\" helper func for annotating column \"Metadata_control\" \"\"\"\n",
    "    ## Only TC, NC, PC are used for constructing the null distribution because of multiple duplicates \n",
    "    if col_annot in [\"TC\", \"NC\", \"PC\"]:\n",
    "        return True\n",
    "    ## else labeled as not controls\n",
    "    elif col_annot in [\"disease_wt\", \"allele\", \"cPC\", \"cNC\"]:\n",
    "        return False\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def add_control_annot(dframe):\n",
    "    \"\"\"annotating column \"Metadata_control\" \"\"\"\n",
    "    if \"Metadata_control\" not in dframe.columns:\n",
    "        dframe[\"Metadata_control\"] = dframe[\"Metadata_node_type\"].apply(\n",
    "            lambda x: control_type_helper(x)\n",
    "        )\n",
    "    return dframe\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    QC functions for filter out imaging wells with low cell counts\n",
    "\"\"\"\n",
    "def drop_low_cc_wells(dframe, cc_thresh, log_file):\n",
    "    # Drop wells with cell counts lower than the threshold\n",
    "    dframe[\"Metadata_Cell_ID\"] = dframe.index\n",
    "    cell_count = (\n",
    "        dframe.groupby([\"Metadata_Plate\", \"Metadata_Well\"])[\"Metadata_Cell_ID\"]\n",
    "        .count()\n",
    "        .reset_index(name=\"Metadata_Cell_Count\")\n",
    "    )\n",
    "    ## get the cell counts per well per plate\n",
    "    dframe = dframe.merge(\n",
    "        cell_count,\n",
    "        on=[\"Metadata_Plate\", \"Metadata_Well\"],\n",
    "    )\n",
    "    dframe_dropped = (\n",
    "        dframe[dframe[\"Metadata_Cell_Count\"] < cc_thresh]\n",
    "    )\n",
    "    ## keep track of the alleles in a log file\n",
    "    log_file.write(f\"Number of wells dropped due to cell counts < {cc_thresh}: {len((dframe_dropped['Metadata_Plate']+dframe_dropped['Metadata_Well']+dframe_dropped['Metadata_gene_allele']).unique())}\\n\")\n",
    "    dframe_dropped = dframe_dropped.drop_duplicates(subset=[\"Metadata_Plate\", \"Metadata_Well\"])\n",
    "    if (dframe_dropped.shape[0] > 0):\n",
    "        for idx in dframe_dropped.index:\n",
    "            log_file.write(f\"{dframe_dropped.loc[idx, 'Metadata_Plate']}, {dframe_dropped.loc[idx, 'Metadata_Well']}:{dframe_dropped.loc[idx, 'Metadata_gene_allele']}\\n\")\n",
    "            # print(f\"{dframe_dropped.loc[idx, 'Metadata_Plate']}, {dframe_dropped.loc[idx, 'Metadata_Well']}:{dframe_dropped.loc[idx, 'Metadata_gene_allele']}\\n\")\n",
    "    ## keep only the wells with cc >= cc_thresh\n",
    "    dframe = (\n",
    "        dframe[dframe[\"Metadata_Cell_Count\"] >= cc_thresh]\n",
    "        .drop(columns=[\"Metadata_Cell_Count\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return dframe\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Utils functions for setting up trn/test sets for classifier training and testing\n",
    "\"\"\"\n",
    "def get_common_plates(dframe1, dframe2):\n",
    "    \"\"\"Helper func: get common plates in two dataframes\"\"\"\n",
    "    plate_list = list(\n",
    "        set(list(dframe1[\"Metadata_Plate\"].unique()))\n",
    "        & set(list(dframe2[\"Metadata_Plate\"].unique()))\n",
    "    )\n",
    "    return plate_list\n",
    "\n",
    "\n",
    "def stratify_by_plate(df_sampled: pd.DataFrame, plate: str):\n",
    "    \"\"\"Stratify dframe by plate\"\"\"\n",
    "    # print(df_sampled.head())\n",
    "    df_sampled_platemap = plate.split(\"_T\")[0]\n",
    "    platemaps = df_sampled[df_sampled[\"Metadata_Plate\"].str.contains(df_sampled_platemap)][\"Metadata_plate_map_name\"].to_list()\n",
    "    assert(len(set(platemaps))==1), \"Only one platemap should be associated with plate: {plate}.\"\n",
    "    platemap = platemaps[0]\n",
    "\n",
    "    # Train on data from same platemap but other plates\n",
    "    df_train = df_sampled[\n",
    "        (df_sampled[\"Metadata_plate_map_name\"] == platemap)\n",
    "        & (df_sampled[\"Metadata_Plate\"] != plate)\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    df_test = df_sampled[df_sampled[\"Metadata_Plate\"] == plate].reset_index(drop=True)\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def get_classifier_features(dframe: pd.DataFrame, feat_type: str):\n",
    "    \"\"\"Helper function to get dframe containing protein or non-protein features\"\"\"\n",
    "    assert feat_type in FEAT_TYPE_SET, f\"ONLY features in {FEAT_TYPE_SET} are allowed\"\n",
    "    feat_col = find_feat_cols(dframe)\n",
    "    meta_col = find_meta_cols(dframe)\n",
    "\n",
    "    if feat_type != \"Morph\":\n",
    "        feat_col = [\n",
    "            i\n",
    "            for i in feat_col\n",
    "            if (feat_type.lower() in i.lower())\n",
    "            and (\"Brightfield\" not in i) ## excluding Brightfield features\n",
    "        ]\n",
    "    else:\n",
    "        feat_col = [\n",
    "            i\n",
    "            for i in feat_col\n",
    "            if (\"GFP\" not in i) and (\"Brightfield\" not in i) and (\"TxControl\" not in i)\n",
    "        ]\n",
    "\n",
    "    dframe = pd.concat([dframe[meta_col], dframe[feat_col]], axis=1)\n",
    "    return dframe\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Implementation of XGBoost Classifier\n",
    "\"\"\"\n",
    "def classifier(df_train, df_test, log_file, target=\"Label\", shuffle=False):\n",
    "    \"\"\"\n",
    "    This function train and test a classifier on the single-cell profiles from ref. and var. alleles.\n",
    "    \"\"\"\n",
    "\n",
    "    feat_col = find_feat_cols(df_train)\n",
    "    feat_col.remove(target)\n",
    "\n",
    "    x_train, y_train = cp.array(df_train[feat_col].to_numpy()), df_train[[target]]\n",
    "    x_test, y_test = cp.array(df_test[feat_col].to_numpy()), df_test[[target]]\n",
    "\n",
    "    num_pos = df_train[df_train[target] == 1].shape[0]\n",
    "    num_neg = df_train[df_train[target] == 0].shape[0]\n",
    "\n",
    "    unique_plates = \",\".join(sorted(df_train['Metadata_Plate'].unique()))\n",
    "    gene_symbols = \",\".join(sorted(df_train['Metadata_symbol'].unique()))\n",
    "    wells = \",\".join(sorted(df_train['Metadata_well_position'].unique()))\n",
    "\n",
    "    if (num_pos == 0) or (num_neg == 0):\n",
    "        log_file.write(f\"Missing positive/negative labels for {gene_symbols} in wells: {wells} from plates {unique_plates}\\n\")\n",
    "        log_file.write(f\"Size of pos: {num_pos}, Size of neg: {num_neg}\\n\")\n",
    "        print(f\"Size of pos: {num_pos}, Size of neg: {num_neg}\")\n",
    "        feat_importances = pd.Series(np.nan, index=df_train[feat_col].columns)\n",
    "        return feat_importances, None, None\n",
    "\n",
    "    scale_pos_weight = num_neg / num_pos\n",
    "\n",
    "    if (scale_pos_weight > 100) or (scale_pos_weight < 0.01):\n",
    "        log_file.write(f\"Extreme class imbalance for {gene_symbols} in wells: {wells} from plates {unique_plates}\\n\")\n",
    "        log_file.write(f\"Scale_pos_weight: {scale_pos_weight}, Size of pos: {num_pos}, Size of neg: {num_neg}\\n\")\n",
    "        print(\n",
    "            f\"Scale_pos_weight: {scale_pos_weight}, Size of pos: {num_pos}, Size of neg: {num_neg}\"\n",
    "        )\n",
    "        feat_importances = pd.Series(np.nan, index=df_train[feat_col].columns)\n",
    "        return feat_importances, None, None\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_train = cp.array(le.fit_transform(y_train))\n",
    "    y_test = cp.array(le.fit_transform(y_test))\n",
    "\n",
    "    if shuffle:\n",
    "        # Create shuffled train labels\n",
    "        y_train_shuff = y_train.copy()\n",
    "        y_train_shuff[\"Label\"] = np.random.permutation(y_train.values)\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        n_estimators=150,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "        learning_rate=0.05,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "    ).fit(x_train, y_train, verbose=False)\n",
    "\n",
    "    # get predictions and scores\n",
    "    pred_score = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    # Return classifier info\n",
    "    info_0 = df_test[df_test[\"Label\"] == 0].iloc[0]\n",
    "    info_1 = df_test[df_test[\"Label\"] == 1].iloc[0]\n",
    "    class_ID = (\n",
    "        info_0[\"Metadata_Plate\"]\n",
    "        + \"_\"\n",
    "        + info_0[\"Metadata_well_position\"]\n",
    "        + \"_\"\n",
    "        + info_1[\"Metadata_well_position\"]\n",
    "    )\n",
    "    classifier_df = pd.DataFrame({\n",
    "        \"Classifier_ID\": [class_ID],\n",
    "        \"Plate\": [info_0[\"Metadata_Plate\"]],\n",
    "        \"trainsize_0\": [sum(y_train.get() == 0)],\n",
    "        \"testsize_0\": [sum(y_test.get() == 0)],\n",
    "        \"well_0\": [info_0[\"Metadata_well_position\"]],\n",
    "        \"allele_0\": [info_0[\"Metadata_gene_allele\"]],\n",
    "        \"trainsize_1\": [sum(y_train.get() == 1)],\n",
    "        \"testsize_1\": [sum(y_test.get() == 1)],\n",
    "        \"well_1\": [info_1[\"Metadata_well_position\"]],\n",
    "        \"allele_1\": [info_1[\"Metadata_gene_allele\"]],\n",
    "    })\n",
    "\n",
    "    # Store feature importance\n",
    "    feat_importances = pd.Series(\n",
    "        model.feature_importances_, index=df_train[feat_col].columns\n",
    "    )\n",
    "\n",
    "    # Return cell-level predictions\n",
    "    cellID = df_test.apply(\n",
    "        lambda row: f\"{row['Metadata_Plate']}_{row['Metadata_well_position']}_{row['Metadata_ImageNumber']}_{row['Metadata_ObjectNumber']}\",\n",
    "        axis=1,\n",
    "    ).to_list()\n",
    "\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"Classifier_ID\": class_ID,\n",
    "        \"CellID\": cellID,\n",
    "        \"Label\": y_test.get(),\n",
    "        \"Prediction\": pred_score,\n",
    "    })\n",
    "\n",
    "    return feat_importances, classifier_df, pred_df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Set up classification workflow with plate_layout of single replicate (single_rep) per plate\n",
    "\"\"\"\n",
    "def experimental_runner(\n",
    "    exp_dframe: pd.DataFrame,\n",
    "    pq_writer,\n",
    "    log_file,\n",
    "    feat_type,\n",
    "    group_key_one=\"Metadata_symbol\",\n",
    "    group_key_two=\"Metadata_gene_allele\",\n",
    "    threshold_key=\"Metadata_node_type\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Reference v.s. Variant experiments\n",
    "    \"\"\"\n",
    "    exp_dframe = get_classifier_features(exp_dframe, feat_type)\n",
    "    feat_cols = find_feat_cols(exp_dframe)\n",
    "    feat_cols = [i for i in feat_cols if i != \"Label\"]\n",
    "\n",
    "    if len(feat_cols) == 0:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    group_list = []\n",
    "    pair_list = []\n",
    "    feat_list = []\n",
    "    info_list = []\n",
    "\n",
    "    log_file.write(f\"Running XGBboost classifiers w/ {feat_type} features on target variants:\\n\")\n",
    "    ## First group the df by reference genes\n",
    "    groups = exp_dframe.groupby(group_key_one).groups\n",
    "    for key in tqdm(groups.keys()):\n",
    "        dframe_grouped = exp_dframe.loc[groups[key]].reset_index(drop=True)\n",
    "        # Ensure this gene has both reference and variants\n",
    "        if dframe_grouped[threshold_key].unique().size < 2:\n",
    "            continue\n",
    "        df_group_one = dframe_grouped[\n",
    "            dframe_grouped[threshold_key] != \"allele\" #== \"disease_wt\", sometimes the reference is misannotated (e.g. KRAS)\n",
    "        ].reset_index(drop=True)\n",
    "        df_group_one[\"Label\"] = 1\n",
    "\n",
    "        ## Then group the gene-specific df by different variant alleles\n",
    "        subgroups = (\n",
    "            dframe_grouped[dframe_grouped[threshold_key] == \"allele\"]\n",
    "            .groupby(group_key_two)\n",
    "            .groups\n",
    "        )\n",
    "        for subkey in subgroups.keys():\n",
    "            df_group_two = dframe_grouped.loc[subgroups[subkey]].reset_index(drop=True)\n",
    "            df_group_two[\"Label\"] = 0\n",
    "            plate_list = get_common_plates(df_group_one, df_group_two)\n",
    "\n",
    "            ## Get ALL the wells for reference gene and variant alleles and pair up all possible combinations\n",
    "            ref_wells = df_group_one[\"Metadata_well_position\"].unique()\n",
    "            var_wells = list(df_group_two[\"Metadata_well_position\"].unique())\n",
    "            ref_var_pairs = [(ref_well, var_well) for ref_well in ref_wells for var_well in var_wells]\n",
    "            df_sampled_ = pd.concat([df_group_one, df_group_two], ignore_index=True)\n",
    "            ## Per each ref-var well pair on the SAME plate, train and test the classifier\n",
    "            for ref_var in ref_var_pairs:\n",
    "                df_sampled = df_sampled_[df_sampled_[\"Metadata_well_position\"].isin(ref_var)]\n",
    "                ## Define the func. for thread_map the plate on the same df_sampled\n",
    "                def classify_by_plate_helper(plate):\n",
    "                    df_train, df_test = stratify_by_plate(df_sampled, plate)\n",
    "                    feat_importances, classifier_info, predictions = classifier(\n",
    "                        df_train, df_test, log_file\n",
    "                    )\n",
    "                    return {plate: [feat_importances, classifier_info, predictions]}\n",
    "                try:\n",
    "                    result = thread_map(classify_by_plate_helper, plate_list)\n",
    "                    pred_list = []\n",
    "                    for res in result:\n",
    "                        if list(res.values())[-1] is not None:\n",
    "                            feat_list.append(list(res.values())[0][0])\n",
    "                            group_list.append(key)\n",
    "                            pair_list.append(f\"{key}_{subkey}\")\n",
    "                            info_list.append(list(res.values())[0][1])\n",
    "                            pred_list.append(list(res.values())[0][2])\n",
    "                        else:\n",
    "                            log_file.write(f\"Skipped classification result for {key}_{subkey}\\n\")\n",
    "                            print(f\"Skipping classification result for {key}_{subkey}...\")\n",
    "                            feat_list.append([None] * len(feat_cols))\n",
    "                            group_list.append(key)\n",
    "                            pair_list.append(f\"{key}_{subkey}\")\n",
    "                            info_list.append([None] * 10)\n",
    "\n",
    "                    cell_preds = pd.concat(pred_list, axis=0)\n",
    "                    cell_preds[\"Metadata_Feature_Type\"] = feat_type\n",
    "                    cell_preds[\"Metadata_Control\"] = False\n",
    "                    table = pa.Table.from_pandas(cell_preds, preserve_index=False)\n",
    "                    pq_writer.write_table(table)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    log_file.write(f\"{key}, {subkey} error: {e}\\n\")\n",
    "        #     break\n",
    "        # break\n",
    "\n",
    "    # Store feature importance\n",
    "    df_feat_one = pd.DataFrame({\"Group1\": group_list, \"Group2\": pair_list})\n",
    "    df_feat_two = pd.DataFrame(feat_list)\n",
    "    df_feat = pd.concat([df_feat_one, df_feat_two], axis=1)\n",
    "    df_feat[\"Metadata_Feature_Type\"] = feat_type\n",
    "    df_feat[\"Metadata_Control\"] = False\n",
    "\n",
    "    # process classifier info\n",
    "    df_result = pd.concat(info_list, ignore_index=True)\n",
    "    df_result[\"Metadata_Control\"] = False\n",
    "    df_result[\"Metadata_Feature_Type\"] = feat_type\n",
    "\n",
    "    log_file.write(f\"Finished running XGBboost classifiers w/ {feat_type} features on target variants.\\n\")\n",
    "    log_file.write(f\"===========================================================================\\n\\n\")\n",
    "    return df_feat, df_result\n",
    "\n",
    "\n",
    "def control_group_runner(\n",
    "    ctrl_dframe: pd.DataFrame,\n",
    "    pq_writer,\n",
    "    log_file,\n",
    "    feat_type,\n",
    "    group_key_one=\"Metadata_gene_allele\",\n",
    "    group_key_two=\"Metadata_plate_map_name\",\n",
    "    group_key_three=\"Metadata_well_position\",\n",
    "    threshold_key=\"Metadata_well_position\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run null control experiments.\n",
    "    \"\"\"\n",
    "    ctrl_dframe = get_classifier_features(ctrl_dframe, feat_type)\n",
    "    feat_cols = find_feat_cols(ctrl_dframe)\n",
    "    feat_cols = [i for i in feat_cols if i != \"Label\"]\n",
    "\n",
    "    if len(feat_cols) == 0:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    group_list = []\n",
    "    pair_list = []\n",
    "    feat_list = []\n",
    "    info_list = []\n",
    "\n",
    "    log_file.write(f\"Running XGBboost classifiers w/ {feat_type} features on control alleles:\\n\")\n",
    "    ## First group the df by reference genes\n",
    "    groups = ctrl_dframe.groupby(group_key_one).groups\n",
    "    for key in tqdm(groups.keys()):\n",
    "        # groupby alleles\n",
    "        print(key)\n",
    "        dframe_grouped = ctrl_dframe.loc[groups[key]].reset_index(drop=True)\n",
    "        \n",
    "        # Skip controls with no replicates\n",
    "        if dframe_grouped[threshold_key].unique().size < 2:\n",
    "            continue\n",
    "\n",
    "        # group by platemap\n",
    "        subgroups = dframe_grouped.groupby(group_key_two).groups\n",
    "        for key_two in subgroups.keys():\n",
    "            dframe_grouped_two = dframe_grouped.loc[subgroups[key_two]].reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "            # If a well is not present on all four plates, drop well\n",
    "            well_count = dframe_grouped_two.groupby([\"Metadata_Well\"])[\n",
    "                \"Metadata_Plate\"\n",
    "            ].nunique()\n",
    "            well_to_drop = well_count[well_count < 4].index\n",
    "            dframe_grouped_two = dframe_grouped_two[\n",
    "                ~dframe_grouped_two[\"Metadata_Well\"].isin(well_to_drop)\n",
    "            ].reset_index(drop=True)\n",
    "\n",
    "            # group by well\n",
    "            sub_sub_groups = dframe_grouped_two.groupby(group_key_three).groups\n",
    "            sampled_pairs = list(combinations(list(sub_sub_groups.keys()), r=2))\n",
    "\n",
    "            for idx1, idx2 in sampled_pairs:\n",
    "                df_group_one = dframe_grouped_two.loc[sub_sub_groups[idx1]].reset_index(\n",
    "                    drop=True\n",
    "                )\n",
    "                df_group_one[\"Label\"] = 1\n",
    "                df_group_two = dframe_grouped_two.loc[sub_sub_groups[idx2]].reset_index(\n",
    "                    drop=True\n",
    "                )\n",
    "                df_group_two[\"Label\"] = 0\n",
    "                df_sampled = pd.concat([df_group_one, df_group_two], ignore_index=True)\n",
    "\n",
    "                try:\n",
    "                    plate_list = get_common_plates(df_group_one, df_group_two)\n",
    "\n",
    "                    def classify_by_plate_helper(plate):\n",
    "                        df_train, df_test = stratify_by_plate(df_sampled, plate)\n",
    "                        feat_importances, classifier_info, predictions = classifier(\n",
    "                            df_train, df_test, log_file\n",
    "                        )\n",
    "                        return {plate: [feat_importances, classifier_info, predictions]}\n",
    "\n",
    "                    result = thread_map(classify_by_plate_helper, plate_list)\n",
    "\n",
    "                    pred_list = []\n",
    "                    for res in result:\n",
    "                        if list(res.values())[-1] is not None:\n",
    "                            feat_list.append(list(res.values())[0][0])\n",
    "                            group_list.append(key)\n",
    "                            pair_list.append(f\"{idx1}_{idx2}\")\n",
    "                            info_list.append(list(res.values())[0][1])\n",
    "                            pred_list.append(list(res.values())[0][2])\n",
    "                        else:\n",
    "                            log_file.write(f\"Skipped classification result for {key}_{key_two}\\n\")\n",
    "                            print(f\"Skipping classification result for {key}_{key_two}...\")\n",
    "                            feat_list.append([None] * len(feat_cols))\n",
    "                            group_list.append(key)\n",
    "                            pair_list.append(f\"{idx1}_{idx2}\")\n",
    "                            info_list.append([None] * 10)\n",
    "\n",
    "                    cell_preds = pd.concat(pred_list, axis=0)\n",
    "                    cell_preds[\"Metadata_Feature_Type\"] = feat_type\n",
    "                    cell_preds[\"Metadata_Control\"] = True\n",
    "                    table = pa.Table.from_pandas(cell_preds, preserve_index=False)\n",
    "                    pq_writer.write_table(table)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    log_file.write(f\"{key}, {key_two} error: {e}, wells per ctrl: {sub_sub_groups}\\n\")\n",
    "            #     break\n",
    "            # break\n",
    "\n",
    "    # Store feature importance\n",
    "    df_feat_one = pd.DataFrame({\"Group1\": group_list, \"Group2\": pair_list})\n",
    "    df_feat_two = pd.DataFrame(feat_list)\n",
    "    df_feat = pd.concat([df_feat_one, df_feat_two], axis=1)\n",
    "    df_feat[\"Metadata_Feature_Type\"] = feat_type\n",
    "    df_feat[\"Metadata_Control\"] = True\n",
    "\n",
    "    # process classifier info\n",
    "    df_result = pd.concat(info_list, ignore_index=True)\n",
    "    df_result[\"Metadata_Control\"] = True\n",
    "    df_result[\"Metadata_Feature_Type\"] = feat_type\n",
    "\n",
    "    log_file.write(f\"Finished running XGBboost classifiers w/ {feat_type} features on control alleles.\\n\")\n",
    "    log_file.write(f\"===========================================================================\\n\\n\")\n",
    "    return df_feat, df_result\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Set up classification workflow with plate_layout of multiple replicates (multi_rep) per plate\n",
    "\"\"\"\n",
    "#######################################\n",
    "# 2. RUN CLASSIFIERS ON CTRL ALLELES\n",
    "# Resampling ctrl wells and run the classifiers on them for the null dist.\n",
    "#######################################\n",
    "def stratify_by_well_pair_ctrl(dframe_grouped_two: pd.DataFrame, well_pair_trn: tuple):\n",
    "    \"\"\"Stratify dframe by ctrl well pairs: one pair for training and one pair for testing\"\"\"\n",
    "    sub_sub_groups = dframe_grouped_two.groupby(\"Metadata_well_position\").groups\n",
    "    assert len(sub_sub_groups.keys()) == 4, f\"Number of wells per plate is not 4: {sub_sub_groups.keys()}\"\n",
    "\n",
    "    ## Train on data from well_pair_trn\n",
    "    df_group_one = dframe_grouped_two.loc[sub_sub_groups[well_pair_trn[0]]].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    df_group_one[\"Label\"] = 1\n",
    "    df_group_two = dframe_grouped_two.loc[sub_sub_groups[well_pair_trn[1]]].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    df_group_two[\"Label\"] = 0\n",
    "    df_sampled = pd.concat([df_group_one, df_group_two], ignore_index=True)\n",
    "    df_train = df_sampled.reset_index(drop=True)\n",
    "\n",
    "    ## Test on data from well_pair_test\n",
    "    well_pair_test = tuple(key for key in sub_sub_groups.keys() if key not in well_pair_trn)\n",
    "    df_group_3 = dframe_grouped_two.loc[sub_sub_groups[well_pair_test[0]]].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    df_group_3[\"Label\"] = 1\n",
    "    df_group_4 = dframe_grouped_two.loc[sub_sub_groups[well_pair_test[1]]].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    df_group_4[\"Label\"] = 0\n",
    "    df_sampled_test = pd.concat([df_group_3, df_group_4], ignore_index=True)\n",
    "    df_test = df_sampled_test.reset_index(drop=True)\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def control_group_runner_fewer_rep(\n",
    "    ctrl_dframe: pd.DataFrame,\n",
    "    pq_writer,\n",
    "    err_logger,\n",
    "    feat_type,\n",
    "    group_key_one=\"Metadata_gene_allele\",\n",
    "    group_key_two=\"Metadata_plate_map_name\",\n",
    "    group_key_three=\"Metadata_well_position\",\n",
    "    threshold_key=\"Metadata_well_position\",\n",
    "    well_count_min=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run classifiers on control alleles.\n",
    "\n",
    "    # df_feat_pro_con, df_result_pro_con = control_group_runner_fewer_rep(df_control, pq_writer=writer, err_logger=err_logger, feat_type=feat_type)\n",
    "    \"\"\"\n",
    "    ctrl_dframe = get_classifier_features(ctrl_dframe, feat_type)\n",
    "    feat_cols = find_feat_cols(ctrl_dframe)\n",
    "    feat_cols = [i for i in feat_cols if i != \"Label\"]\n",
    "\n",
    "    if len(feat_cols) == 0:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    group_list = []\n",
    "    pair_list = []\n",
    "    feat_list = []\n",
    "    info_list = []\n",
    "\n",
    "    err_logger.write(f\"Logging errors when running control experiments w/ {feat_type} features:\\n\")\n",
    "    ## first we group the cells from the same Metadata_gene_allele\n",
    "    groups = ctrl_dframe.groupby(group_key_one).groups\n",
    "    for key in tqdm(groups.keys()):\n",
    "        ## groupby alleles\n",
    "        dframe_grouped = ctrl_dframe.loc[groups[key]].reset_index(drop=True)\n",
    "        # Skip controls with no replicates\n",
    "        if dframe_grouped[threshold_key].unique().size < 2:\n",
    "            continue\n",
    "        ## group by platemap\n",
    "        subgroups = dframe_grouped.groupby(group_key_two).groups\n",
    "        for key_two in subgroups.keys():\n",
    "            ## for each platemap\n",
    "            dframe_grouped_two = dframe_grouped.loc[subgroups[key_two]].reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "            ## If a well is not present on all four plates, drop well\n",
    "            ## ONLY used when we have enough TECHNICAL-REPLICATE plates!!!\n",
    "            if well_count_min is not None:\n",
    "                well_count = dframe_grouped_two.groupby([\"Metadata_Well\"])[\n",
    "                    \"Metadata_Plate\"\n",
    "                ].nunique()\n",
    "                well_to_drop = well_count[well_count < well_count_min].index\n",
    "                dframe_grouped_two = dframe_grouped_two[\n",
    "                    ~dframe_grouped_two[\"Metadata_Well\"].isin(well_to_drop)\n",
    "                ].reset_index(drop=True)\n",
    "\n",
    "            ## group by well\n",
    "            sub_sub_groups = dframe_grouped_two.groupby(group_key_three).groups\n",
    "            sampled_pairs = list(combinations(list(sub_sub_groups.keys()), r=2))\n",
    "            ## juxtapose each pair of wells against each other                \n",
    "            try:\n",
    "                def classify_by_well_pair_helper(df_sampled: pd.DataFrame, well_pair: tuple, log_file=err_logger):\n",
    "                    \"\"\"Helper func to run classifiers in parallel\"\"\"\n",
    "                    df_train, df_test = stratify_by_well_pair_ctrl(df_sampled, well_pair)\n",
    "                    feat_importances, classifier_info, predictions = classifier(\n",
    "                        df_train, df_test, log_file\n",
    "                    )\n",
    "                    return {f\"trn_{well_pair[0]}_{well_pair[1]}\": [feat_importances, classifier_info, predictions]}\n",
    "                \n",
    "                ## Bind df_sampled to the helper function\n",
    "                classify_by_well_pair_bound = partial(classify_by_well_pair_helper, dframe_grouped_two)\n",
    "                result = thread_map(classify_by_well_pair_bound, sampled_pairs)\n",
    "                pred_list = []\n",
    "                for res in result:\n",
    "                    if list(res.values())[-1] is not None:\n",
    "                        feat_list.append(list(res.values())[0][0])\n",
    "                        group_list.append(key)\n",
    "                        pair_list.append(list(res.keys())[0])\n",
    "                        info_list.append(list(res.values())[0][1])\n",
    "                        pred_list.append(list(res.values())[0][2])\n",
    "                    else:\n",
    "                        err_logger.write(f\"Skipped classification result for {key}_{key_two}\\n\")\n",
    "                        print(f\"Skipping classification result for {key}_{key_two}...\")\n",
    "                        feat_list.append([None] * len(feat_cols))\n",
    "                        group_list.append(key)\n",
    "                        pair_list.append(list(res.keys())[0])\n",
    "                        info_list.append([None] * 10)\n",
    "\n",
    "                cell_preds = pd.concat(pred_list, axis=0)\n",
    "                cell_preds[\"Metadata_Feature_Type\"] = feat_type\n",
    "                cell_preds[\"Metadata_Control\"] = True\n",
    "                table = pa.Table.from_pandas(cell_preds, preserve_index=False)\n",
    "                pq_writer.write_table(table)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                err_logger.write(f\"{key}, {key_two} error: {e}, wells per ctrl: {sub_sub_groups}\\n\")\n",
    "\n",
    "    # Store feature importance\n",
    "    df_feat_one = pd.DataFrame({\"Group1\": group_list, \"Group2\": pair_list})\n",
    "    df_feat_two = pd.DataFrame(feat_list)\n",
    "    df_feat = pd.concat([df_feat_one, df_feat_two], axis=1)\n",
    "    df_feat[\"Metadata_Feature_Type\"] = feat_type\n",
    "    df_feat[\"Metadata_Control\"] = True\n",
    "\n",
    "    # process classifier info\n",
    "    df_result = pd.concat(info_list, ignore_index=True)\n",
    "    df_result[\"Metadata_Control\"] = True\n",
    "    df_result[\"Metadata_Feature_Type\"] = feat_type\n",
    "\n",
    "    err_logger.write(f\"Logging errors when running control experiments w/ {feat_type} features finished.\\n\")\n",
    "    err_logger.write(f\"==============================================================================\\n\\n\")\n",
    "    return df_feat, df_result\n",
    "    \n",
    "\n",
    "#######################################\n",
    "# 3. RUN CLASSIFIERS ON VAR-REF ALLELES\n",
    "# Construct 4-fold CV on var-vs-ref wells and run the classifiers on them.\n",
    "#######################################\n",
    "def stratify_by_well_pair_exp(df_sampled: pd.DataFrame, well_pair_list: list):\n",
    "    \"\"\"\n",
    "        Stratify dframe by plate\n",
    "        df_sampled: the data frame containing both ref. and var. alleles, each tested in 4 wells\n",
    "        well_pair: a list of well pairs containing a ref. and a var. allele, with 1st pair for test and the rest pairs for training\n",
    "    \"\"\"\n",
    "    ## Training on the rest three wells out of four\n",
    "    df_train = df_sampled[\n",
    "        (df_sampled[\"Metadata_well_position\"].isin([well for pair in well_pair_list[1:] for well in pair]))\n",
    "    ].reset_index(drop=True)\n",
    "    ## Testing on the well_pair\n",
    "    df_test = df_sampled[\n",
    "        df_sampled[\"Metadata_well_position\"].isin(well_pair_list[0])\n",
    "    ].reset_index(drop=True)\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def experimental_runner_plate_rep(\n",
    "    exp_dframe: pd.DataFrame,\n",
    "    pq_writer,\n",
    "    err_logger,\n",
    "    feat_type,\n",
    "    group_key_one=\"Metadata_symbol\",\n",
    "    group_key_two=\"Metadata_gene_allele\",\n",
    "    threshold_key=\"Metadata_node_type\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Reference v.s. Variant experiments run on the same plate without tech. dups\n",
    "\n",
    "    # df_feat_pro_exp, df_result_pro_exp = experimental_runner_plate_rep(df_exp, pq_writer=writer, protein=True)\n",
    "    \"\"\"\n",
    "    exp_dframe = get_classifier_features(exp_dframe, feat_type)\n",
    "    feat_cols = find_feat_cols(exp_dframe)\n",
    "    feat_cols = [i for i in feat_cols if i != \"Label\"]\n",
    "\n",
    "    if len(feat_cols) == 0:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    group_list = []\n",
    "    pair_list = []\n",
    "    feat_list = []\n",
    "    info_list = []\n",
    "\n",
    "    err_logger.write(f\"Logging errors when running real experiments w/ {feat_type} features:\\n\")\n",
    "    groups = exp_dframe.groupby(group_key_one).groups\n",
    "    for key in tqdm(groups.keys()):\n",
    "        dframe_grouped = exp_dframe.loc[groups[key]].reset_index(drop=True)\n",
    "\n",
    "        # Ensure this gene has both reference and variants\n",
    "        if dframe_grouped[threshold_key].unique().size < 2:\n",
    "            continue\n",
    "\n",
    "        df_group_one = dframe_grouped[\n",
    "            dframe_grouped[\"Metadata_node_type\"] != \"allele\" #== \"disease_wt\", sometimes the reference is misannotated (e.g. KRAS)\n",
    "        ].reset_index(drop=True)\n",
    "        df_group_one[\"Label\"] = 1\n",
    "        ref_al_wells = df_group_one[\"Metadata_well_position\"].unique()\n",
    "        \n",
    "        subgroups = (\n",
    "            dframe_grouped[dframe_grouped[\"Metadata_node_type\"] == \"allele\"]\n",
    "            .groupby(group_key_two)\n",
    "            .groups\n",
    "        )\n",
    "\n",
    "        for subkey in subgroups.keys():\n",
    "            df_group_two = dframe_grouped.loc[subgroups[subkey]].reset_index(drop=True)\n",
    "            df_group_two[\"Label\"] = 0\n",
    "            var_al_wells = df_group_two[\"Metadata_well_position\"].unique()\n",
    "            df_sampled = pd.concat([df_group_one, df_group_two], ignore_index=True)\n",
    "\n",
    "            if len(ref_al_wells) < 4:\n",
    "                # ref_al_wells = np.random.choice(ref_al_wells, size=4)\n",
    "                err_logger.write(f\"{key}, {subkey} pair DOES NOT enough ref. alleles! Ref. allele wells in parquet: {ref_al_wells}\\n\")\n",
    "                continue\n",
    "            if len(var_al_wells) < 4:\n",
    "                # var_al_wells = np.random.choice(var_al_wells, size=4)\n",
    "                err_logger.write(f\"{key}, {subkey} pair DOES NOT enough var. alleles! Var. allele wells in parquet: {var_al_wells}\\n\")\n",
    "                continue\n",
    "                \n",
    "            well_pair_list = list(zip(ref_al_wells, var_al_wells))\n",
    "            well_pair_nested_list = [[well_pair_list[i]] + well_pair_list[:i] + well_pair_list[i+1:] for i in range(len(well_pair_list))]\n",
    "            ## try run classifier\n",
    "            try:\n",
    "                def classify_by_well_pair_exp_helper(df_sampled: pd.DataFrame, well_pair_list: list, log_file=err_logger):\n",
    "                    \"\"\"Helper func to run classifiers in parallel for var-ref alleles\"\"\"\n",
    "                    df_train, df_test = stratify_by_well_pair_exp(df_sampled, well_pair_list)\n",
    "                    feat_importances, classifier_info, predictions = classifier(\n",
    "                        df_train, df_test, log_file\n",
    "                    )\n",
    "                    well_pair = well_pair_list[0]\n",
    "                    return {f\"test_{well_pair[0]}_{well_pair[1]}\": [feat_importances, classifier_info, predictions]}\n",
    "\n",
    "                ## Bind df_sampled to the helper function\n",
    "                classify_by_well_pair_bound = partial(classify_by_well_pair_exp_helper, df_sampled)\n",
    "                result = thread_map(classify_by_well_pair_bound, well_pair_nested_list)\n",
    "                \n",
    "                pred_list = []\n",
    "                for res in result:\n",
    "                    if list(res.values())[-1] is not None:\n",
    "                        feat_list.append(list(res.values())[0][0])\n",
    "                        group_list.append(key)\n",
    "                        pair_list.append(f\"{key}_{subkey}\")\n",
    "                        info_list.append(list(res.values())[0][1])\n",
    "                        pred_list.append(list(res.values())[0][2])\n",
    "                    else:\n",
    "                        err_logger.write(f\"Skipped classification result for {key}_{subkey}\\n\")\n",
    "                        print(f\"Skipping classification result for {key}_{subkey}...\")\n",
    "                        feat_list.append([None] * len(feat_cols))\n",
    "                        group_list.append(key)\n",
    "                        pair_list.append(f\"{key}_{subkey}\")\n",
    "                        info_list.append([None] * 10)\n",
    "\n",
    "                cell_preds = pd.concat(pred_list, axis=0)\n",
    "                cell_preds[\"Metadata_Feature_Type\"] = feat_type\n",
    "                cell_preds[\"Metadata_Control\"] = False\n",
    "                table = pa.Table.from_pandas(cell_preds, preserve_index=False)\n",
    "                pq_writer.write_table(table)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                err_logger.write(f\"{key}, {subkey} error: {e}\")\n",
    "\n",
    "    ### Store feature importance\n",
    "    df_feat_one = pd.DataFrame({\"Group1\": group_list, \"Group2\": pair_list})\n",
    "    df_feat_two = pd.DataFrame(feat_list)\n",
    "    df_feat = pd.concat([df_feat_one, df_feat_two], axis=1)\n",
    "    df_feat[\"Metadata_Feature_Type\"] = feat_type\n",
    "    df_feat[\"Metadata_Control\"] = False\n",
    "\n",
    "    # process classifier info\n",
    "    df_result = pd.concat(info_list, ignore_index=True)\n",
    "    df_result[\"Metadata_Control\"] = False\n",
    "    df_result[\"Metadata_Feature_Type\"] = feat_type\n",
    "\n",
    "    err_logger.write(f\"Logging errors when running real experiments w/ {feat_type} features finished.\\n\")\n",
    "    err_logger.write(f\"===========================================================================\\n\\n\")\n",
    "    return df_feat, df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edae8e80-10a1-41b1-af75-8ca483ec04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classify_workflow(\n",
    "    input_path: str,\n",
    "    feat_output_path: str,\n",
    "    info_output_path: str,\n",
    "    preds_output_path: str,\n",
    "    cc_threshold: int,\n",
    "    plate_layout: str,\n",
    "    use_gpu: Union[str, None] = \"0,1\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run workflow for single-cell classification\n",
    "    \"\"\"\n",
    "    assert plate_layout in (\"single_rep\", \"multi_rep\"), f\"Incorrect plate_layout: {plate_layout}, only 'single_rep' and 'multi_rep' allowed.\"\n",
    "\n",
    "    if use_gpu is not None:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = use_gpu\n",
    "\n",
    "    # Initialize parquet for cell-level predictions\n",
    "    if os.path.exists(preds_output_path):\n",
    "        os.remove(preds_output_path)\n",
    "    \n",
    "    ## create a log file\n",
    "    logfile_path = os.path.join('/'.join(preds_output_path.split(\"/\")[:-1]), \"classify.log\")\n",
    "\n",
    "    ## output the prediction.parquet\n",
    "    schema = pa.schema([\n",
    "        (\"Classifier_ID\", pa.string()),\n",
    "        (\"CellID\", pa.string()),\n",
    "        (\"Label\", pa.int64()),\n",
    "        (\"Prediction\", pa.float32()),\n",
    "        (\"Metadata_Feature_Type\", pa.string()),\n",
    "        (\"Metadata_Control\", pa.bool_()),\n",
    "    ])\n",
    "    writer = pq.ParquetWriter(preds_output_path, schema, compression=\"gzip\")\n",
    "\n",
    "    # Add CellID column\n",
    "    dframe = (\n",
    "        pl.scan_parquet(input_path)\n",
    "        .with_columns(\n",
    "            pl.concat_str(\n",
    "                [\n",
    "                    \"Metadata_Plate\",\n",
    "                    \"Metadata_well_position\",\n",
    "                    \"Metadata_ImageNumber\",\n",
    "                    \"Metadata_ObjectNumber\",\n",
    "                ],\n",
    "                separator=\"_\",\n",
    "            ).alias(\"Metadata_CellID\")\n",
    "        )\n",
    "        .collect()\n",
    "        .to_pandas()\n",
    "    )\n",
    "    ## select the feature columns\n",
    "    feat_col = find_feat_cols(dframe)\n",
    "\n",
    "    try:\n",
    "        assert (\n",
    "            ~np.isnan(dframe[feat_col]).any().any()\n",
    "        ), \"Dataframe contains no NaN features.\"\n",
    "        assert (\n",
    "            np.isfinite(dframe[feat_col]).all().all()\n",
    "        ), \"Dataframe contains finite feature values.\"\n",
    "    except AssertionError:\n",
    "        dframe = remove_nan_infs_columns(dframe)\n",
    "\n",
    "    ## Filter rows with NaN Metadata\n",
    "    dframe = dframe[~dframe[\"Metadata_well_position\"].isna()]\n",
    "    dframe = add_control_annot(dframe)\n",
    "    dframe = dframe[~dframe[\"Metadata_control\"].isna()]\n",
    "\n",
    "    ## store the classifier feat_importance and classification_res\n",
    "    feat_import_dfs, class_res_dfs = [], [] \n",
    "    ## Split data into experimental df with var and ref alleles\n",
    "    df_exp = dframe[~dframe[\"Metadata_control\"].astype(\"bool\")].reset_index(drop=True)\n",
    "    with open(logfile_path, \"w\") as log_file:\n",
    "        log_file.write(f\"===============================================================================================================================================================\\n\")\n",
    "        log_file.write(\"Dropping low cell count wells in ref. vs variant alleles:\\n\")\n",
    "        print(\"Dropping low cell count wells in ref. vs variant alleles:\")\n",
    "        df_exp = drop_low_cc_wells(df_exp, cc_threshold, log_file)\n",
    "        log_file.write(f\"===============================================================================================================================================================\\n\\n\")\n",
    "        # Check the plate_layout for the correct classification set-up\n",
    "        if (plate_layout==\"single_rep\"):\n",
    "            ## If the plate_layout is single_rep, with only one well per allele on a single plate\n",
    "            ## we can only get the control_df with the control labels\n",
    "            df_control = dframe[dframe[\"Metadata_control\"].astype(\"bool\")].reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "            # Remove any remaining TC from analysis\n",
    "            df_control = df_control[df_control[\"Metadata_node_type\"] != \"TC\"].reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "            log_file.write(\"Dropping low cell count wells in ONLY the control alleles on the same plate:\\n\")\n",
    "            print(\"Dropping low cell count wells in ONLY the control alleles on the same plate:\\n\")\n",
    "            # Filter out wells with fewer than the cell count threhsold\n",
    "            df_control = drop_low_cc_wells(df_control, cc_threshold, log_file)\n",
    "            print(\"Check ctrl df:\")\n",
    "            display(df_control)\n",
    "\n",
    "            for feat in FEAT_TYPE_SET:\n",
    "                print(feat)\n",
    "                df_feat_pro_con, df_result_pro_con = control_group_runner(\n",
    "                    df_control, pq_writer=writer, log_file=log_file, feat_type=feat\n",
    "                )\n",
    "                df_feat_pro_exp, df_result_pro_exp = experimental_runner(\n",
    "                    df_exp, pq_writer=writer, log_file=log_file, feat_type=feat\n",
    "                )\n",
    "                if (df_feat_pro_con.shape[0] > 0):\n",
    "                    feat_import_dfs += [df_feat_pro_con, df_feat_pro_exp]\n",
    "                    class_res_dfs += [df_result_pro_con, df_result_pro_exp]\n",
    "        else:\n",
    "            ## If the plate_layout is multi_rep, with multiple wells per allele on a single plate\n",
    "            ## we can get control_df with every possible allele on the same plate\n",
    "            ## As long as it is not a TC:\n",
    "            df_control = dframe[dframe[\"Metadata_node_type\"] != \"TC\"].reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "            log_file.write(\"Dropping low cell count wells in every possible allele that could be used as controls:\\n\")\n",
    "            print(\"Dropping low cell count wells in every possible allele that could be used as controls:\\n\")\n",
    "            # Filter out wells with fewer than the cell count threhsold\n",
    "            df_control = drop_low_cc_wells(df_control, cc_threshold, log_file)\n",
    "            \n",
    "            for feat in FEAT_TYPE_SET:\n",
    "                df_feat_pro_con, df_result_pro_con = control_group_runner_fewer_rep(\n",
    "                    df_control, pq_writer=writer, err_logger=log_file, feat_type=feat\n",
    "                )\n",
    "                df_feat_pro_exp, df_result_pro_exp = experimental_runner_plate_rep(\n",
    "                    df_exp, pq_writer=writer, err_logger=log_file, feat_type=feat\n",
    "                )\n",
    "\n",
    "                if (df_feat_pro_con.shape[0] > 0):\n",
    "                    feat_import_dfs += [df_feat_pro_con, df_feat_pro_exp]\n",
    "                    class_res_dfs += [df_result_pro_con, df_result_pro_exp]\n",
    "\n",
    "        ## Close the parquest writer\n",
    "        writer.close()\n",
    "\n",
    "    # Concatenate results for both protein and non-protein\n",
    "    df_feat = pd.concat(\n",
    "        feat_import_dfs, ignore_index=True\n",
    "    )\n",
    "    \n",
    "    df_result = pd.concat(\n",
    "        class_res_dfs, ignore_index=True\n",
    "    )\n",
    "    df_result = df_result.drop_duplicates()\n",
    "\n",
    "    # Write out feature importance and classifier info\n",
    "    df_feat.to_csv(feat_output_path, index=False)\n",
    "    df_result.to_csv(info_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642c8e53-4ca1-48be-9af2-f46b2c86ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_classify_workflow(\n",
    "#     input_path: str,\n",
    "#     feat_output_path: str,\n",
    "#     info_output_path: str,\n",
    "#     preds_output_path: str,\n",
    "#     cc_threshold: int,\n",
    "#     plate_layout: str,\n",
    "#     use_gpu: Union[str, None] = \"0,1\",\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Run workflow for single-cell classification\n",
    "#     \"\"\"\n",
    "#     assert plate_layout in (\"single_rep\", \"multi_rep\"), f\"Incorrect plate_layout: {plate_layout}, only 'single_rep' and 'multi_rep' allowed.\"\n",
    "\n",
    "#     if use_gpu is not None:\n",
    "#         os.environ[\"CUDA_VISIBLE_DEVICES\"] = use_gpu\n",
    "\n",
    "#     # Initialize parquet for cell-level predictions\n",
    "#     if os.path.exists(preds_output_path):\n",
    "#         os.remove(preds_output_path)\n",
    "    \n",
    "#     ## create a log file\n",
    "#     logfile_path = os.path.join('/'.join(preds_output_path.split(\"/\")[:-1]), \"classify.log\")\n",
    "\n",
    "#     ## output the prediction.parquet\n",
    "#     schema = pa.schema([\n",
    "#         (\"Classifier_ID\", pa.string()),\n",
    "#         (\"CellID\", pa.string()),\n",
    "#         (\"Label\", pa.int64()),\n",
    "#         (\"Prediction\", pa.float32()),\n",
    "#         (\"Metadata_Feature_Type\", pa.string()),\n",
    "#         (\"Metadata_Control\", pa.bool_()),\n",
    "#     ])\n",
    "#     writer = pq.ParquetWriter(preds_output_path, schema, compression=\"gzip\")\n",
    "\n",
    "#     # Add CellID column\n",
    "#     dframe = (\n",
    "#         pl.scan_parquet(input_path)\n",
    "#         .with_columns(\n",
    "#             pl.concat_str(\n",
    "#                 [\n",
    "#                     \"Metadata_Plate\",\n",
    "#                     \"Metadata_well_position\",\n",
    "#                     \"Metadata_ImageNumber\",\n",
    "#                     \"Metadata_ObjectNumber\",\n",
    "#                 ],\n",
    "#                 separator=\"_\",\n",
    "#             ).alias(\"Metadata_CellID\")\n",
    "#         ).filter(\n",
    "#             pl.col(\"Metadata_symbol\")==\"KRAS\"\n",
    "#         )\n",
    "#         .collect()\n",
    "#         .to_pandas()\n",
    "#     )\n",
    "#     ## select the feature columns\n",
    "#     feat_col = find_feat_cols(dframe)\n",
    "\n",
    "#     try:\n",
    "#         assert (\n",
    "#             ~np.isnan(dframe[feat_col]).any().any()\n",
    "#         ), \"Dataframe contains no NaN features.\"\n",
    "#         assert (\n",
    "#             np.isfinite(dframe[feat_col]).all().all()\n",
    "#         ), \"Dataframe contains finite feature values.\"\n",
    "#     except AssertionError:\n",
    "#         dframe = remove_nan_infs_columns(dframe)\n",
    "\n",
    "#     ## Filter rows with NaN Metadata\n",
    "#     dframe = dframe[~dframe[\"Metadata_well_position\"].isna()]\n",
    "#     dframe = add_control_annot(dframe)\n",
    "#     dframe = dframe[~dframe[\"Metadata_control\"].isna()]\n",
    "\n",
    "#     ## store the classifier feat_importance and classification_res\n",
    "#     feat_import_dfs, class_res_dfs = [], [] \n",
    "#     ## Split data into experimental df with var and ref alleles\n",
    "#     df_exp = dframe[~dframe[\"Metadata_control\"].astype(\"bool\")].reset_index(drop=True)\n",
    "\n",
    "#     # print(df_exp)\n",
    "#     with open(logfile_path, \"w\") as log_file:\n",
    "#         log_file.write(f\"===============================================================================================================================================================\\n\")\n",
    "#         log_file.write(\"Dropping low cell count wells in ref. vs variant alleles:\\n\")\n",
    "#         print(\"Dropping low cell count wells in ref. vs variant alleles:\")\n",
    "#         df_exp = drop_low_cc_wells(df_exp, cc_threshold, log_file)\n",
    "#         log_file.write(f\"===============================================================================================================================================================\\n\\n\")\n",
    "#         display(df_exp)\n",
    "#         display(df_exp[df_exp[\"Metadata_gene_allele\"]==\"KRAS\"])\n",
    "#         # Check the plate_layout for the correct classification set-up\n",
    "#         if (plate_layout==\"single_rep\"):\n",
    "#             ## If the plate_layout is single_rep, with only one well per allele on a single plate\n",
    "#             ## we can only get the control_df with the control labels\n",
    "#             # df_control = dframe[dframe[\"Metadata_control\"].astype(\"bool\")].reset_index(\n",
    "#             #     drop=True\n",
    "#             # )\n",
    "#             # # Remove any remaining TC from analysis\n",
    "#             # df_control = df_control[df_control[\"Metadata_node_type\"] != \"TC\"].reset_index(\n",
    "#             #     drop=True\n",
    "#             # )\n",
    "#             # log_file.write(\"Dropping low cell count wells in ONLY the control alleles on the same plate:\\n\")\n",
    "#             # print(\"Dropping low cell count wells in ONLY the control alleles on the same plate:\\n\")\n",
    "#             # # Filter out wells with fewer than the cell count threhsold\n",
    "#             # df_control = drop_low_cc_wells(df_control, cc_threshold, log_file)\n",
    "#             # print(\"Check ctrl df:\")\n",
    "#             # print(df_control)\n",
    "\n",
    "#             for feat in FEAT_TYPE_SET:\n",
    "#                 print(feat)\n",
    "#                 # df_feat_pro_con, df_result_pro_con = control_group_runner(\n",
    "#                 #     df_control, pq_writer=writer, log_file=log_file, feat_type=feat\n",
    "#                 # )\n",
    "#                 df_feat_pro_exp, df_result_pro_exp = experimental_runner(\n",
    "#                     df_exp, pq_writer=writer, log_file=log_file, feat_type=feat\n",
    "#                 )\n",
    "#                 feat_import_dfs += [df_feat_pro_con, df_feat_pro_exp]\n",
    "#                 class_res_dfs += [df_result_pro_con, df_result_pro_exp]\n",
    "#         else:\n",
    "#             ## If the plate_layout is multi_rep, with multiple wells per allele on a single plate\n",
    "#             ## we can get control_df with every possible allele on the same plate\n",
    "#             ## As long as it is not a TC:\n",
    "#             df_control = dframe[dframe[\"Metadata_node_type\"] != \"TC\"].reset_index(\n",
    "#                 drop=True\n",
    "#             )\n",
    "#             log_file.write(\"Dropping low cell count wells in every possible allele that could be used as controls:\\n\")\n",
    "#             print(\"Dropping low cell count wells in every possible allele that could be used as controls:\\n\")\n",
    "#             # Filter out wells with fewer than the cell count threhsold\n",
    "#             df_control = drop_low_cc_wells(df_control, cc_threshold, log_file)\n",
    "            \n",
    "#             for feat in FEAT_TYPE_SET:\n",
    "#                 # df_feat_pro_con, df_result_pro_con = control_group_runner_fewer_rep(\n",
    "#                 #     df_control, pq_writer=writer, err_logger=log_file, feat_type=feat\n",
    "#                 # )\n",
    "#                 df_feat_pro_exp, df_result_pro_exp = experimental_runner_plate_rep(\n",
    "#                     df_exp, pq_writer=writer, err_logger=log_file, feat_type=feat\n",
    "#                 )\n",
    "#                 feat_import_dfs += [df_feat_pro_con, df_feat_pro_exp]\n",
    "#                 class_res_dfs += [df_result_pro_con, df_result_pro_exp]\n",
    "\n",
    "#         ## Close the parquest writer\n",
    "#         writer.close()\n",
    "\n",
    "#     # Concatenate results for both protein and non-protein\n",
    "#     df_feat = pd.concat(\n",
    "#         feat_import_dfs, ignore_index=True\n",
    "#     )\n",
    "    \n",
    "#     df_result = pd.concat(\n",
    "#         class_res_dfs, ignore_index=True\n",
    "#     )\n",
    "#     df_result = df_result.drop_duplicates()\n",
    "\n",
    "#     # Write out feature importance and classifier info\n",
    "#     df_feat.to_csv(feat_output_path, index=False)\n",
    "#     df_result.to_csv(info_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd04a75-e98c-45ce-81c3-1e23ad847891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping low cell count wells in ref. vs variant alleles:\n",
      "Dropping low cell count wells in every possible allele that could be used as controls:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eeed596e2d34c4489dc88902a1277a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/90 [00:03<04:47,  3.23s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40026643bec6458f920a8d22d1d78d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 2/90 [00:05<03:41,  2.52s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4d877524c249aa8a763835d7925fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 3/90 [00:07<03:30,  2.42s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e791a60e3a4c0b9670c1d0b32102a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 4/90 [00:09<03:18,  2.31s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6446be508444de9de097d324325b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 5/90 [00:12<03:29,  2.47s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d409ea8904549c984931df06e130d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 6/90 [00:14<03:15,  2.33s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75206483fc444d1973753c75049bc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 7/90 [00:17<03:32,  2.56s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3bbf93887e48708992aed5919d53d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 8/90 [00:20<03:31,  2.57s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5960678f9014816a94e841d2b861bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 9/90 [00:22<03:15,  2.41s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae252a002574d2c9755fe64cf105a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 10/90 [00:24<03:10,  2.38s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8f36ef22b74469a8c2c6edfd0831ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 11/90 [00:26<03:07,  2.37s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e598373f9e441289025f711f286cb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 12/90 [00:28<02:59,  2.30s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bef378dfdc4b269e2e9cefd1991967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 13/90 [00:31<02:56,  2.29s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2faf5f62a67c4c0d9e2a30919185eb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 14/90 [00:33<02:56,  2.33s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2163e594479c46959ee16a516ad0cf17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 15/90 [00:36<02:55,  2.34s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934cf8f01d694dee89ec444d5049d643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 16/90 [00:38<02:51,  2.32s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2df9d6b6e84b45a21eea212dea5b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 17/90 [00:40<02:46,  2.29s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4071df568330416a8fc0a13f536de0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 18/90 [00:42<02:37,  2.18s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e17510712b8474c8644b623ada8f44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 19/90 [00:45<02:47,  2.36s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a6349a17e543f39d523d048627b50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 20/90 [00:47<02:48,  2.41s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443580534c31462db7aee30bc4fa3c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|       | 21/90 [00:49<02:41,  2.35s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876122d0d69a48fdad6b8ee67150b05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 22/90 [00:52<02:36,  2.31s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97428bcbcc3488883032f0a41b190d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 23/90 [00:54<02:37,  2.36s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed590101d5d4f2a8974c8296687db47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 24/90 [00:57<02:37,  2.38s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c35078f5cf436087da132272268017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 25/90 [00:59<02:37,  2.42s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246d9c22af9540549c7f2a5840ecd66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 26/90 [01:01<02:27,  2.31s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bd62a0fd56407386f83f6786c36cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 27/90 [01:04<02:28,  2.36s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4621016505403f92bb5c1705b79ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 28/90 [01:06<02:24,  2.32s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d062e79b07af4db9a5b16b31e1c6f2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 29/90 [01:08<02:18,  2.28s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be11359ce7df4e56ae412e48830aef37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 30/90 [01:10<02:16,  2.28s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be89c389cafb4af4842e529b43bd62fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 31/90 [01:13<02:15,  2.30s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15aea19d312b49e5af8b4399869d4622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 32/90 [01:15<02:16,  2.36s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b728c6b85fa8466a91bb2318c35fecbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 33/90 [01:17<02:13,  2.34s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b7006c391d4bfa988c02f4554dbb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 34/90 [01:20<02:13,  2.39s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208972e99bae451aa6d1709d88faf4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 35/90 [01:23<02:16,  2.48s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb99efb0af104cf58868e9e65eeb4de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 36/90 [01:25<02:09,  2.41s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8740398264a49539f5eeb3ad8548989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 37/90 [01:27<02:00,  2.28s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0892cab63ec74a8da16075b03fcbfd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 38/90 [01:29<01:57,  2.27s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2744ca3097d44f1a2c4928cc346fa1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 39/90 [01:32<02:03,  2.42s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96e37a05fb045a5bc9fd0cdaeefae6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 40/90 [01:34<01:56,  2.32s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43c90a392604fe0b3a4d86a39b03f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 41/90 [01:37<02:00,  2.45s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1433bb33d3bc4684a026e3e55844e352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 42/90 [01:39<02:01,  2.53s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2984a2af7f834078b91cb9c2e096cf8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 43/90 [01:42<01:59,  2.54s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a1a8659057490e8b1059e9c61fd11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 44/90 [01:45<02:02,  2.67s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b38404cffc642a3bb9d0fef10350e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 45/90 [01:47<01:54,  2.55s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d408a8fbe8a463eb6476825b677caf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 46/90 [01:50<01:54,  2.61s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195938d450cf48489c443d963c055899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 47/90 [01:53<01:54,  2.67s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d86a413c67a4f44941592c72c8c171a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 48/90 [01:56<01:53,  2.71s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb5935478f04f25b189781f54ed0244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 48/90 [01:58<01:43,  2.47s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = \"../outputs/batch_profiles/2025_05_23_Batch_17/profiles_tcdropped_filtered_var_mad_outlier_featselect_filtcells.parquet\"\n",
    "feat_output_path = \"../outputs/tmp\"\n",
    "info_output_path = \"../outputs/tmp\"\n",
    "preds_output_path = \"../outputs/tmp/predict.parquet\"\n",
    "cc_threshold = 20\n",
    "plate_layout = \"multi_rep\"\n",
    "\n",
    "run_classify_workflow(\n",
    "    input_path,\n",
    "    feat_output_path,\n",
    "    info_output_path,\n",
    "    preds_output_path,\n",
    "    cc_threshold,\n",
    "    plate_layout\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varchamp",
   "language": "python",
   "name": "varchamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
